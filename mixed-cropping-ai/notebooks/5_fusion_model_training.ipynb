{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a99a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n",
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342edd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all datasets and pre-trained models...\n",
      "All datasets and models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load All Datasets and Pre-trained Models\n",
    "print(\"Loading all datasets and pre-trained models...\")\n",
    "\n",
    "# --- Load Datasets ---\n",
    "try:\n",
    "    env_df = pd.read_csv('../data/simulated/environmental_data.csv', parse_dates=['timestamp'])\n",
    "    soil_df = pd.read_csv('../data/simulated/soil_microbe_data.csv')\n",
    "    image_dir = '../data/raw/leaf_images/'\n",
    "    if not os.path.exists(image_dir) or not os.listdir(image_dir): raise FileNotFoundError\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: A required data file or directory is missing. Please ensure all previous steps are complete. Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Load Models ---\n",
    "try:\n",
    "    env_model_path = '../backend/models/environmental_yield_model.h5'\n",
    "    soil_model_path = '../backend/models/soil_microbe_yield_model.h5'\n",
    "    vision_model_path = '../backend/models/leaf_vision_model.h5'\n",
    "    soil_preprocessor_path = '../backend/models/soil_data_preprocessor.joblib'\n",
    "\n",
    "    # FIX: Added compile=False to prevent deserialization errors.\n",
    "    # We only need the model architecture and weights, not the old training configuration.\n",
    "    env_model = load_model(env_model_path, compile=False)\n",
    "    soil_model = load_model(soil_model_path, compile=False)\n",
    "    vision_model = load_model(vision_model_path, compile=False)\n",
    "    soil_preprocessor = joblib.load(soil_preprocessor_path)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load a model or preprocessor. Make sure all previous notebooks ran successfully. Details: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"All datasets and models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "136dc91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for the fusion model...\n",
      "Data preparation complete.\n",
      "Shape of Environmental Input (X_env): (15, 168, 4)\n",
      "Shape of Soil Input (X_soil): (15, 9)\n",
      "Shape of Vision Input (X_vision): (15, 224, 224, 3)\n",
      "Shape of Target (y): (15,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Prepare Data for Multi-Input Model\n",
    "print(\"\\nPreparing data for the fusion model...\")\n",
    "\n",
    "# We need to create a single dataset where each row corresponds to one final yield prediction\n",
    "# and has pointers to the environmental sequence, soil data, and a representative leaf image.\n",
    "\n",
    "# Let's use the soil_df as our base, as it has one row per plot_id and contains the final yield.\n",
    "fusion_df = soil_df.copy()\n",
    "\n",
    "# 1. Prepare Environmental Data Sequences\n",
    "SEQUENCE_LENGTH = 168 # Must be the same as in notebook 2\n",
    "env_sequences = []\n",
    "# For each plot, we'll just grab the first sequence of environmental data for simplicity\n",
    "for plot_id in fusion_df['plot_id']:\n",
    "    plot_env_data = env_df[env_df['plot_id'] == plot_id].head(SEQUENCE_LENGTH)\n",
    "    # Note: In a real scenario, you might average sequences or use a more complex sampling method.\n",
    "    env_sequences.append(plot_env_data[['temperature', 'humidity', 'soil_moisture', 'soil_ph']].values)\n",
    "\n",
    "X_env = np.array(env_sequences)\n",
    "# Normalize the environmental data just like we did in notebook 2\n",
    "# This requires a new scaler or reusing the old one if saved. For simplicity, we'll fit a new one.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "env_scaler = MinMaxScaler()\n",
    "# We need to reshape for the scaler, then reshape back\n",
    "X_env_reshaped = X_env.reshape(-1, X_env.shape[-1])\n",
    "X_env_scaled_reshaped = env_scaler.fit_transform(X_env_reshaped)\n",
    "X_env = X_env_scaled_reshaped.reshape(X_env.shape)\n",
    "\n",
    "\n",
    "# 2. Prepare Soil Data\n",
    "X_soil_raw = fusion_df.drop(columns=['yield_kg_ha', 'plot_id', 'crop_mix'])\n",
    "X_soil = soil_preprocessor.transform(X_soil_raw)\n",
    "\n",
    "\n",
    "# 3. Prepare Image Data\n",
    "# We need to associate each plot with a representative image.\n",
    "# For this simulation, we'll randomly pick an image from a relevant category.\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "image_paths = []\n",
    "vision_classes = sorted(os.listdir(image_dir)) # Get class names like 'arecanut_healthy', etc.\n",
    "\n",
    "for index, row in fusion_df.iterrows():\n",
    "    # Create a plausible image category, e.g., 'banana_healthy'\n",
    "    crop_name = row['primary_crop'].lower().replace(' ', '_')\n",
    "    # Randomly assign a status\n",
    "    status = random.choice(['healthy', 'pest_damage', 'nutrient_deficiency'])\n",
    "    plausible_class = f\"{crop_name}_{status}\"\n",
    "    \n",
    "    # If that class folder doesn't exist, default to the first available one\n",
    "    if plausible_class not in vision_classes:\n",
    "        plausible_class = vision_classes[0]\n",
    "        \n",
    "    class_path = os.path.join(image_dir, plausible_class)\n",
    "    random_image_name = random.choice(os.listdir(class_path))\n",
    "    image_paths.append(os.path.join(class_path, random_image_name))\n",
    "\n",
    "# Function to load and preprocess a single image\n",
    "def preprocess_image(path):\n",
    "    img = load_img(path, target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array /= 255.0 # Rescale\n",
    "    return img_array\n",
    "\n",
    "X_vision = np.array([preprocess_image(p) for p in image_paths])\n",
    "\n",
    "\n",
    "# 4. Prepare Target Variable\n",
    "y = fusion_df['yield_kg_ha'].values\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "print(\"Shape of Environmental Input (X_env):\", X_env.shape)\n",
    "print(\"Shape of Soil Input (X_soil):\", X_soil.shape)\n",
    "print(\"Shape of Vision Input (X_vision):\", X_vision.shape)\n",
    "print(\"Shape of Target (y):\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e2b5d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building the Fusion Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_38\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_38\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ environmental_input │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">168</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ soil_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vision_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ env_feature_extrac… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">84,234</span> │ environmental_in… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ soil_feature_extra… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">11,616</span> │ soil_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vision_feature_ext… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ vision_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1362</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ env_feature_extr… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ soil_feature_ext… │\n",
       "│                     │                   │            │ vision_feature_e… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">174,464</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ yield_prediction    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ environmental_input │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m168\u001b[0m, \u001b[38;5;34m4\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ soil_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vision_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ env_feature_extrac… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │     \u001b[38;5;34m84,234\u001b[0m │ environmental_in… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ soil_feature_extra… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m11,616\u001b[0m │ soil_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vision_feature_ext… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │  \u001b[38;5;34m2,257,984\u001b[0m │ vision_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1362\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ env_feature_extr… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ soil_feature_ext… │\n",
       "│                     │                   │            │ vision_feature_e… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m174,464\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ yield_prediction    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,536,619</span> (9.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,536,619\u001b[0m (9.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">182,785</span> (714.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m182,785\u001b[0m (714.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,353,834</span> (8.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,353,834\u001b[0m (8.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4: Build the Fusion Model\n",
    "print(\"\\nBuilding the Fusion Model...\")\n",
    "\n",
    "# Freeze the layers of all base models so their weights are not changed during fusion training.\n",
    "env_model.trainable = False\n",
    "soil_model.trainable = False\n",
    "vision_model.trainable = False\n",
    "\n",
    "# Define the three input layers for our new model\n",
    "input_env = Input(shape=(SEQUENCE_LENGTH, X_env.shape[2]), name='environmental_input')\n",
    "input_soil = Input(shape=(X_soil.shape[1],), name='soil_input')\n",
    "input_vision = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3), name='vision_input')\n",
    "\n",
    "# --- FIX STARTS HERE ---\n",
    "# Rebuild the branches to extract features from the pre-trained models.\n",
    "\n",
    "# For the Sequential models, we create new models containing all layers except the final output layer.\n",
    "env_branch = Sequential(env_model.layers[:-1], name=\"env_feature_extractor\")\n",
    "soil_branch = Sequential(soil_model.layers[:-1], name=\"soil_feature_extractor\")\n",
    "\n",
    "# The vision model was built with the Functional API, so we can create a branch from its input and intermediate layer.\n",
    "vision_branch = Model(inputs=vision_model.input, outputs=vision_model.layers[-2].output, name=\"vision_feature_extractor\")\n",
    "# --- FIX ENDS HERE ---\n",
    "\n",
    "# Pass the main inputs through their respective feature-extracting branches\n",
    "env_features = env_branch(input_env)\n",
    "soil_features = soil_branch(input_soil)\n",
    "vision_features = vision_branch(input_vision)\n",
    "\n",
    "# Concatenate (fuse) the features from all three branches\n",
    "combined_features = Concatenate()([env_features, soil_features, vision_features])\n",
    "\n",
    "# Add our new \"head\" model on top of the fused features to make the final prediction\n",
    "x = Dense(128, activation='relu')(combined_features)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "final_output = Dense(1, name='yield_prediction')(x)\n",
    "\n",
    "# Create the final fusion model\n",
    "fusion_model = Model(inputs=[input_env, input_soil, input_vision], outputs=final_output)\n",
    "\n",
    "# Compile the fusion model\n",
    "fusion_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "fusion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "975242e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the fusion model...\n",
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step - loss: 1557691.6250 - mean_absolute_error: 1120.3102 - val_loss: 2368218.2500 - val_mean_absolute_error: 1388.8422\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - loss: 1521740.8750 - mean_absolute_error: 1105.7889 - val_loss: 2318738.7500 - val_mean_absolute_error: 1372.5343\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - loss: 1489623.3750 - mean_absolute_error: 1092.5873 - val_loss: 2270357.2500 - val_mean_absolute_error: 1356.3999\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - loss: 1464979.6250 - mean_absolute_error: 1080.6664 - val_loss: 2222358.5000 - val_mean_absolute_error: 1340.2046\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - loss: 1434936.6250 - mean_absolute_error: 1068.5778 - val_loss: 2175855.5000 - val_mean_absolute_error: 1324.3882\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step - loss: 1404646.8750 - mean_absolute_error: 1057.3197 - val_loss: 2129849.5000 - val_mean_absolute_error: 1308.5012\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - loss: 1363413.8750 - mean_absolute_error: 1036.5168 - val_loss: 2082332.1250 - val_mean_absolute_error: 1291.9313\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 526ms/step - loss: 1334574.8750 - mean_absolute_error: 1020.5544 - val_loss: 2032743.0000 - val_mean_absolute_error: 1274.4244\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step - loss: 1290493.3750 - mean_absolute_error: 1005.8581 - val_loss: 1981959.8750 - val_mean_absolute_error: 1256.2333\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461ms/step - loss: 1268212.6250 - mean_absolute_error: 993.4582 - val_loss: 1929809.0000 - val_mean_absolute_error: 1237.2623\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - loss: 1233659.5000 - mean_absolute_error: 982.3016 - val_loss: 1875683.0000 - val_mean_absolute_error: 1217.2552\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step - loss: 1239042.0000 - mean_absolute_error: 972.6101 - val_loss: 1819657.0000 - val_mean_absolute_error: 1196.2103\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - loss: 1127528.6250 - mean_absolute_error: 936.6400 - val_loss: 1761276.6250 - val_mean_absolute_error: 1173.8972\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - loss: 1078852.3750 - mean_absolute_error: 891.4234 - val_loss: 1699513.8750 - val_mean_absolute_error: 1149.7816\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step - loss: 1029470.4375 - mean_absolute_error: 891.5262 - val_loss: 1634202.5000 - val_mean_absolute_error: 1123.7404\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499ms/step - loss: 1008467.1875 - mean_absolute_error: 865.2348 - val_loss: 1564640.6250 - val_mean_absolute_error: 1095.4402\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - loss: 899544.3125 - mean_absolute_error: 801.4656 - val_loss: 1492644.0000 - val_mean_absolute_error: 1065.3999\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474ms/step - loss: 862164.0000 - mean_absolute_error: 791.8679 - val_loss: 1418709.1250 - val_mean_absolute_error: 1033.6976\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - loss: 791644.7500 - mean_absolute_error: 732.1191 - val_loss: 1342845.8750 - val_mean_absolute_error: 1000.1824\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444ms/step - loss: 735786.8125 - mean_absolute_error: 718.5180 - val_loss: 1265641.6250 - val_mean_absolute_error: 964.9522\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - loss: 749336.3125 - mean_absolute_error: 719.5335 - val_loss: 1187584.0000 - val_mean_absolute_error: 928.0588\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - loss: 683193.9375 - mean_absolute_error: 655.1569 - val_loss: 1109184.5000 - val_mean_absolute_error: 889.5664\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460ms/step - loss: 616777.3750 - mean_absolute_error: 603.2211 - val_loss: 1030042.1875 - val_mean_absolute_error: 849.0565\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - loss: 651993.9375 - mean_absolute_error: 623.8784 - val_loss: 950570.6875 - val_mean_absolute_error: 806.4611\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - loss: 650928.3125 - mean_absolute_error: 636.0975 - val_loss: 871485.8125 - val_mean_absolute_error: 761.9183\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - loss: 538315.9375 - mean_absolute_error: 559.6670 - val_loss: 793576.6875 - val_mean_absolute_error: 715.5530\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - loss: 481384.7812 - mean_absolute_error: 521.1028 - val_loss: 717968.5000 - val_mean_absolute_error: 667.7544\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - loss: 515781.5312 - mean_absolute_error: 539.6140 - val_loss: 645444.3750 - val_mean_absolute_error: 618.7676\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462ms/step - loss: 387987.0938 - mean_absolute_error: 463.5899 - val_loss: 577129.1250 - val_mean_absolute_error: 571.6797\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - loss: 200701.9375 - mean_absolute_error: 344.3293 - val_loss: 512680.1562 - val_mean_absolute_error: 544.6229\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - loss: 279701.5312 - mean_absolute_error: 389.5757 - val_loss: 452530.5000 - val_mean_absolute_error: 517.0935\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - loss: 222546.4219 - mean_absolute_error: 349.6305 - val_loss: 398298.2500 - val_mean_absolute_error: 489.7849\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 361196.2500 - mean_absolute_error: 515.5825 - val_loss: 351498.5312 - val_mean_absolute_error: 463.7235\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - loss: 310219.3438 - mean_absolute_error: 430.7873 - val_loss: 311015.0312 - val_mean_absolute_error: 438.5725\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - loss: 326426.3750 - mean_absolute_error: 449.0468 - val_loss: 277048.5938 - val_mean_absolute_error: 414.8857\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505ms/step - loss: 138540.8281 - mean_absolute_error: 289.1144 - val_loss: 250473.0781 - val_mean_absolute_error: 394.0250\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - loss: 191475.4375 - mean_absolute_error: 363.7715 - val_loss: 228946.2969 - val_mean_absolute_error: 374.9847\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - loss: 287153.8438 - mean_absolute_error: 478.0609 - val_loss: 211489.1719 - val_mean_absolute_error: 357.5366\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - loss: 115369.5391 - mean_absolute_error: 309.1432 - val_loss: 198780.0469 - val_mean_absolute_error: 343.2236\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - loss: 229171.8750 - mean_absolute_error: 350.8280 - val_loss: 189497.9531 - val_mean_absolute_error: 338.6544\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step - loss: 238117.8750 - mean_absolute_error: 416.3207 - val_loss: 182823.4844 - val_mean_absolute_error: 341.8959\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - loss: 146958.7812 - mean_absolute_error: 350.6059 - val_loss: 177867.0469 - val_mean_absolute_error: 344.4138\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 580ms/step - loss: 230229.2500 - mean_absolute_error: 399.1203 - val_loss: 173870.8125 - val_mean_absolute_error: 346.5297\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step - loss: 511482.7188 - mean_absolute_error: 579.5417 - val_loss: 171775.1719 - val_mean_absolute_error: 347.3744\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - loss: 180664.2500 - mean_absolute_error: 366.3587 - val_loss: 170308.8750 - val_mean_absolute_error: 347.7306\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - loss: 252783.0000 - mean_absolute_error: 447.1519 - val_loss: 169171.3594 - val_mean_absolute_error: 347.8156\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - loss: 238763.3750 - mean_absolute_error: 393.8122 - val_loss: 169125.1094 - val_mean_absolute_error: 346.9908\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - loss: 221880.6250 - mean_absolute_error: 424.9455 - val_loss: 169629.5156 - val_mean_absolute_error: 345.7271\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 177641.0156 - mean_absolute_error: 376.2828 - val_loss: 171168.4375 - val_mean_absolute_error: 343.7383\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - loss: 196071.4844 - mean_absolute_error: 364.1370 - val_loss: 173753.7031 - val_mean_absolute_error: 341.0520\n",
      "Fusion model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train the Fusion Model\n",
    "print(\"\\nTraining the fusion model...\")\n",
    "\n",
    "# NOTE: Our dataset is very small (15 plots), so training will be quick but not very robust.\n",
    "# This code demonstrates the process; real-world use would require much more data.\n",
    "history = fusion_model.fit(\n",
    "    [X_env, X_soil, X_vision], # A list of the three input datasets\n",
    "    y,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Fusion model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ec054a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the final fusion model...\n",
      "Fusion model saved successfully to: ../backend/models/fusion_yield_model.h5\n",
      "Environmental data scaler saved successfully to: ../backend/models/env_data_scaler.joblib\n",
      "\n",
      "This is the final, unified model for your backend API.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save the Final Fusion Model\n",
    "print(\"\\nSaving the final fusion model...\")\n",
    "\n",
    "model_dir = '../backend/models/'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, 'fusion_yield_model.h5')\n",
    "scaler_path = os.path.join(model_dir, 'env_data_scaler.joblib')\n",
    "\n",
    "# Save the model\n",
    "fusion_model.save(model_path)\n",
    "print(f\"Fusion model saved successfully to: {model_path}\")\n",
    "\n",
    "# Save the scaler used for the environmental data\n",
    "joblib.dump(env_scaler, scaler_path)\n",
    "print(f\"Environmental data scaler saved successfully to: {scaler_path}\")\n",
    "\n",
    "print(\"\\nThis is the final, unified model for your backend API.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
